激活函数几点性质要求：
1：连续可导（有限点不可导允许）非线性函数
2：尽可能简单，有利于训练
3：导数值的值域在一个合适的区间，不能太大或者太小
不然会影响训练的稳定性和效率



relu作为激活函数的好处
1：计算量小
2：不易出现梯度消失
3：能使一部分神经元的输出为0，网络具有了稀疏性，减少了参数的互相依存关系，缓解过拟合的发生



防止神经网络过拟合的常用方法
1：获取更多的训练数据（数据增强）
2：减少网络容量
3：提前停止（当训练集的拟合优度衡量指标提升不明显时即可停止）
4：添加权重正则化
5：添加dropout
6：批归一化（调整了数据的分布，让每一层的输出归一化到了mean=0 and var=1的分布，优点是学习更快，对初始值不敏感，抑制过拟合）



lstm对比rnn的优势在于可以避免梯度爆炸或者消失



CNN（卷积神经网络）
比如因子数18，5天为窗口，
每一份数据都是18*1一天的数据，是一维的！和图片的二位数据不一样，这里的滚动（卷积）也是沿着时间一直往后
但是滚动的窗口是18*5的过滤器（5是kernel size卷积核的大小）
卷积核的数量可以任意选择代表你希望有多少个特征被学习到，步长也是任意的，每次往后推进多长
然后池化层也和图片的没有区别
