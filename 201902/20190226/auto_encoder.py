# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1czQKiwpzmRlDoIuu007te3JF4BeprhRr
"""

from __future__ import division, print_function, absolute_import

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)

#training parameters
learning_rate = 0.01
num_steps = 30000
batch_size = 256

display_step = 1000
examples_to_show = 10

#network parameters
num_hidden_1 = 20 #1st layer num features
num_hidden_2 = 10 #2nd layer num features (the latent dim)
num_input = 784

#tf Graph input(only pictures)
X = tf.placeholder("float", [None, num_input])

weights = {
    "encoder_h1": tf.Variable(tf.random_normal([num_input, num_hidden_1])),
    "encoder_h2": tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),
    "decoder_h1": tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),
    "decoder_h2": tf.Variable(tf.random_normal([num_hidden_1, num_input])),
}

biases = {
    "encoder_b1": tf.Variable(tf.random_normal([num_hidden_1])),
    "encoder_b2": tf.Variable(tf.random_normal([num_hidden_2])),
    "decoder_b1": tf.Variable(tf.random_normal([num_hidden_1])),
    "decoder_b2": tf.Variable(tf.random_normal([num_input])),
}

#building the encoder

def encoder(x):
  #encoder hidden layer with sigmoid activation #1
  layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights["encoder_h1"]),
                   biases["encoder_b1"]))
  #encoder hidden layer with sigmoid activation #2
  layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights["encoder_h2"]),
                      biases["encoder_b2"]))
  
  return layer_2

#building the decoder
def decoder(x):
  #decoder hidden layer with sigmoid activation #1
  layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights["decoder_h1"]),
      biases["decoder_b1"]))
  #decoder hidden layer with sigmoid activation #2
  layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights["decoder_h2"]),
      biases["decoder_b2"]))
  
  return layer_2

#construct model
encoder_op = encoder(X)
decoder_op = decoder(encoder_op)

#prediction
y_pred = decoder_op
#targets(labels) are the input data
y_true = X

#define loss and optimizer, minimize the squared error
loss = tf.reduce_mean(tf.pow(y_true-y_pred, 2))
optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)

#initialize the variable (assign their default value)
init = tf.global_variables_initializer()

#start training
#start a new tf session
sess = tf.Session()

#run the initializer
sess.run(init)

#training
for i in range(1, num_steps+1):
  #prepare data
  #get the next batch of mnist data
  batch_x, _ = mnist.train.next_batch(batch_size)
  #run optimization op (backprop) and cost op(to get loss value)
  _, l = sess.run([optimizer, loss], feed_dict={X:batch_x})
  #display logs per step
  if i % display_step == 0 or i ==1:
    print("step %i: minibatch loss:%.3f"%(i, l))

#testing
#encode and decode images from test set and visualize their reconstruction
n = 4
canvas_orig = np.empty((28*n, 28*n))
canvas_recon = np.empty((28*n, 28*n))
for i in range(n):
  #mnist test set
  batch_x, _ = mnist.test.next_batch(n)
  #encode and decode the digit image
  g = sess.run(decoder_op, feed_dict={X:batch_x})

  #display original images
  for j in range(n):
    #draw the generated digits
    canvas_orig[i*28:(i+1)*28, j*28:(j+1)*28] = batch_x[j].reshape([28, 28])
  #display reconstructed images
  for j in range(n):
    #draw the generated digits
    canvas_recon[i*28:(i+1)*28, j*28:(j+1)*28] = g[j].reshape([28, 28])

print("original images")
plt.figure(figsize=(n, n))
plt.imshow(canvas_orig, origin="upper", cmap="gray")
plt.show()

print("Reconstructed Images")
plt.figure(figsize=(n, n))
plt.imshow(canvas_recon, origin="upper", cmap="gray")
plt.show()